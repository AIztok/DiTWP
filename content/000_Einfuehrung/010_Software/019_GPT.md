---
title: 019_GPT
draft: false
tags:
  - "#GPT"
---
# Allgemein 

GPT (Generative Pre-trained Transformer) ist eine Reihe von KI-Modellen, die von OpenAI entwickelt wurden und auf der Transformer-Architektur basieren. GPT-Modelle sind sogenannte Large Language Models (LLMs), die darauf trainiert sind, menschenähnliche Texte zu generieren und komplexe Aufgaben zu lösen. Sie sind in der Lage, Informationen aus einer Vielzahl von Quellen zu verarbeiten und auf Basis dieser Informationen Texte zu erstellen.

# Wie funktioniert es?

GPT (Generative Pre-trained Transformer) und verwandte Modelle verwenden spezifische architektonische Prinzipien der neuronalen Netze, insbesondere basierend auf Transformer-Architekturen. Hier sind die wesentlichen Komponenten und Konzepte:

1. **Neuronale Netze**:
    
    - **Perzeptron**: Der grundlegendste Baustein eines neuronalen Netzes, der Eingaben in eine Ausgabe transformiert.
    - **Feedforward-Neuronale Netze**: Diese Netzwerke bestehen aus mehreren Schichten von Neuronen, bei denen die Daten in eine Richtung von den Eingaben zu den Ausgaben fließen.
    - **Convolutional Neural Networks (CNNs)**: Weit verbreitet in der Bildverarbeitung. Sie nutzen Faltungsschichten, um Merkmale aus den Eingabedaten zu extrahieren.
    - **Recurrent Neural Networks (RNNs)**: Entwickelt für sequentielle Daten, da sie eine Rückkopplungsschleife haben, die Informationen über Zeit speichern kann. LSTMs (Long Short-Term Memory) sind eine spezielle Art von RNNs, die Langzeitabhängigkeiten besser handhaben können.
    
1. **Transformers**:
    
    - **Self-Attention Mechanism**: Das Kernstück der Transformer-Architektur, das ermöglicht, dass jedes Element einer Eingabesequenz Informationen von jedem anderen Element dieser Sequenz aufnimmt. Dies wird durch die Berechnung von "Aufmerksamkeitsgewichten" für jedes Paar von Elementen erreicht.
    - **Encoder-Decoder-Architektur**: In vielen Anwendungen (wie z.B. Übersetzungsaufgaben) bestehen Transformer-Modelle aus einem Encoder, der die Eingabesequenz verarbeitet, und einem Decoder, der die Ausgabesequenz generiert. GPT-Modelle verwenden jedoch nur den Decoder-Teil für textgenerative Aufgaben.
    - **Positionskodierung**: Da Transformatoren keine inhärente Reihenfolge in ihren Eingaben haben, wird eine Positionskodierung hinzugefügt, um die Positionsinformation der Token in der Sequenz zu vermitteln.
    
1. **GPT-Spezifika**:
    
    - **Unidirektionaler Ansatz**: GPT-Modelle lesen den Text sequentiell von links nach rechts, was bedeutet, dass jedes Token nur Informationen von vorhergehenden Token nutzen kann.
    - **Pre-Training und Fine-Tuning**: Die Modelle werden zuerst auf großen Textkorpora vortrainiert (Pre-Training), um eine allgemeine Sprachrepräsentation zu lernen, und anschließend auf spezifische Aufgaben mit spezifischeren Daten abgestimmt (Fine-Tuning).

Die Kombination dieser Technologien ermöglicht es GPT-Modellen, Kontextinformationen effektiv zu nutzen und kohärente und (meistens / oft) sinnvolle Texte zu generieren.


Video zu neuronalen Netzen:

<iframe width="560" height="315" src="https://www.youtube.com/embed/aircAruvnKk?si=7r9px_ecDTqtGrNV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Video zu dem was sind Transformers und wie die funktionieren:

<iframe width="560" height="315" src="https://www.youtube.com/embed/wjZofJX0v4M?si=9Ai0OZZPS-spTqiY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# Funktioniert es?

LLMs bzw. GPTs sind mit Vorsicht zu genießen. Durchs Halluzinieren und Begrenzung von Tokens sind die Ergebnisse immer / noch zu hinterfragen bzw. durch unsere Eingreifen auszubessern.

<iframe width="560" height="315" src="https://www.youtube.com/embed/cfqtFvWOfg0?si=jl3j2OEe0xnU5jqb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# Wie werden wir GPT bei der LVA nutzen?

LLMs haben sich als besonders nützlich bzw. finden eine breite Anwendung in folgenden Gebieten:
- als Assistenten zur Abfrage von größeren Datenbanken, Dokumenten usw.
- zum schreiben von Programmen, oder besser gesagt Code-blocks

[[013_Grasshopper#Grasshopper und LLMs]] und [[411_VO]]